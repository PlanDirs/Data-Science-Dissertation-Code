{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b6331c",
   "metadata": {},
   "source": [
    "#### data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19ec605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 22\n",
    "X = np.random.rand(n_samples, n_features) * 100\n",
    "y = np.random.rand(n_samples, 3) * 10  # 生成三个回归目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99304d4b",
   "metadata": {},
   "source": [
    "#### train validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c294c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分训练集为新的训练集和验证集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 划分训练集为新的训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c13f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: (640, 22)\n",
      "validation dataset: (160, 22)\n",
      "test dataset: (200, 22)\n"
     ]
    }
   ],
   "source": [
    "# 打印划分后的数据集大小\n",
    "print(\"train dataset:\", X_train.shape)\n",
    "print(\"validation dataset:\", X_val.shape)\n",
    "print(\"test dataset:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b359446",
   "metadata": {},
   "source": [
    "#### change the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db1fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f1d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3ee22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset from the tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Specify the batch size\n",
    "batch_size = 64\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685b628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e160bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b418a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1388b9d7",
   "metadata": {},
   "source": [
    "#### fitness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c348533d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def MLPNN_fitness_function(params):\n",
    "\n",
    "    # Defince the parameters\n",
    "    hidden_size = int(params[0])\n",
    "    learning_rate = params[1]\n",
    "    activation = [nn.Sigmoid(), nn.ReLU(), nn.Tanh()][int(params[2])]\n",
    "    batch_size = [32, 64, 128, 256][int(params[3])]\n",
    "    hidden_size1 = int(params[4])\n",
    "    hidden_size2 = int(params[5])\n",
    "    hidden_size3 = int(params[6])\n",
    "    hidden_size4 = int(params[7])\n",
    "    hidden_size5 = int(params[8])\n",
    "    hidden_size6 = int(params[9])\n",
    "    hidden_size7 = int(params[10])\n",
    "    hidden_size8 = int(params[11])\n",
    "    epoch = [100, 200, 500, 1000, 2000, 5000][int(params[12])]\n",
    "    weight_decay = params[13]\n",
    "    \n",
    "    # Create a TensorDataset from the tensors\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Specify the batch size\n",
    "    batch_size = batch_size\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    #hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5]\n",
    "    def get_hidden_nodes(hidden_size):\n",
    "        if hidden_size == 1:\n",
    "            hidden_sizes = [hidden_size1]\n",
    "        elif hidden_size == 2:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2]\n",
    "        elif hidden_size == 3:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3]\n",
    "        elif hidden_size == 4:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4]\n",
    "        elif hidden_size == 5:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5]\n",
    "        elif hidden_size == 6:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, hidden_size6]\n",
    "        elif hidden_size == 7:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, hidden_size6, hidden_size7]\n",
    "        elif hidden_size == 8:\n",
    "            hidden_sizes = [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, hidden_size6, hidden_size7, hidden_size8]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported hidden_size. Please choose a value between 1 and 5.\")\n",
    "            \n",
    "        #print(hidden_sizes)\n",
    "        return hidden_sizes\n",
    "    \n",
    "    hidden_sizes = get_hidden_nodes(hidden_size)\n",
    "    \n",
    "    \n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_sizes, output_size, activation=activation):\n",
    "            super(MLP, self).__init__()\n",
    "\n",
    "            # Create a list to store the layers\n",
    "            layers = []\n",
    "\n",
    "            # Add the input layer\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            layers.append(activation)\n",
    "\n",
    "            # Add the hidden layers\n",
    "            for i in range(1, len(hidden_sizes)):\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "                layers.append(activation)\n",
    "\n",
    "            # Add the output layer\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "            # Create a sequential model using the layers list\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.model(x)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 初始化模型和优化器\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    model = MLP(input_size, hidden_sizes, output_size, activation)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_epochs = epoch\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            # Clear gradients from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute model predictions\n",
    "            predictions = model(X_batch)\n",
    "            #print('here is prediction：', predictions.shape)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            #print('here is loss：', loss.shape)\n",
    "\n",
    "            # Backpropagation: compute gradients of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimization: update model parameters using the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the total loss for this epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #print(total_loss)\n",
    "        # Calculate the average loss for this epoch\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        #print(average_loss)\n",
    "\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "            #print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_mse = 0.0\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            # Forward pass: compute model predictions\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Compute the loss (assuming you have defined the loss function as 'criterion')\n",
    "            mse = criterion(predictions, batch_y)\n",
    "\n",
    "            # Accumulate the total loss for this validation set\n",
    "            total_mse += mse.item()\n",
    "\n",
    "        # Calculate the average loss for the entire validation set\n",
    "        validation_loss = total_mse / len(val_loader)\n",
    "\n",
    "        # Print the average loss for this validation set\n",
    "        #print(f\"Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409598d",
   "metadata": {},
   "source": [
    "#input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "#output_size = y_train.shape[1]\n",
    "learning_rate = 0.001\n",
    "activation = [nn.ReLU(), nn.Sigmoid(), nn.Tanh()][0]\n",
    "batch_size = 64\n",
    "epoch = 100\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243e6ee",
   "metadata": {},
   "source": [
    "result1 = MLPNN_fitness_function(hidden_size, learning_rate, activation, batch_size, 128, 128, 128, 64, 32, 64, 32, 64, epoch, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101c616",
   "metadata": {},
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cab167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbebdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a72dc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m ga(function\u001b[38;5;241m=\u001b[39mMLPNN_fitness_function, dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, variable_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     31\u001b[0m            variable_type_mixed  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m]) ,variable_boundaries\u001b[38;5;241m=\u001b[39mvarbound, \n\u001b[0;32m     32\u001b[0m            algorithm_parameters\u001b[38;5;241m=\u001b[39malgorithm_param, function_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 运行遗传算法进行优化\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 获得优化的超参数组合\u001b[39;00m\n\u001b[0;32m     38\u001b[0m best_params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\geneticalgorithm\\geneticalgorithm.py:303\u001b[0m, in \u001b[0;36mgeneticalgorithm.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m     var[i]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_bound[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\u001b[38;5;241m*\u001b[39m\\\n\u001b[0;32m    299\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_bound[i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_bound[i][\u001b[38;5;241m0\u001b[39m])    \n\u001b[0;32m    300\u001b[0m     solo[i]\u001b[38;5;241m=\u001b[39mvar[i]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 303\u001b[0m obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[0;32m    304\u001b[0m solo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim]\u001b[38;5;241m=\u001b[39mobj\n\u001b[0;32m    305\u001b[0m pop[p]\u001b[38;5;241m=\u001b[39msolo\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\geneticalgorithm\\geneticalgorithm.py:542\u001b[0m, in \u001b[0;36mgeneticalgorithm.sim\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[43mfunc_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuntimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FunctionTimedOut:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgiven function is not applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\func_timeout\\dafunc.py:86\u001b[0m, in \u001b[0;36mfunc_timeout\u001b[1;34m(timeout, func, args, kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     85\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m---> 86\u001b[0m \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m stopException \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thread\u001b[38;5;241m.\u001b[39mis_alive():\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:1057\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:1073\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1074\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "\n",
    "\n",
    "varbound = np.array([[1, 8],               # number of hidden layer\n",
    "                     [0.0001, 0.2],        # learning rate\n",
    "                     [0, 2],               # activation function  0表示Sigmoid，1表示ReLU，2表示Tanh       \n",
    "                     [0, 3],               # batch size\n",
    "                     [10, 200],            # hidden layer 1\n",
    "                     [10, 200],            # hidden layer 2\n",
    "                     [10, 200],            # hidden layer 3\n",
    "                     [10, 200],            # hidden layer 4\n",
    "                     [10, 200],            # hidden layer 5\n",
    "                     [10, 200],            # hidden layer 6\n",
    "                     [10, 200],            # hidden layer 7\n",
    "                     [10, 200],            # hidden layer 8      \n",
    "                     [0, 5],               # epoch\n",
    "                     [0, 0.01]             # L2 norm\n",
    "                     ])            \n",
    "\n",
    "\n",
    "\n",
    "# 创建遗传算法对象\n",
    "algorithm_param = {'max_num_iteration': 10, 'population_size': 10, 'elit_ratio': 0.01,\n",
    "                   'parents_portion': 0.3, 'crossover_probability': 0.5, 'mutation_probability': 0.1, \n",
    "                   'crossover_type': 'one_point', 'max_iteration_without_improv': None}\n",
    "model = ga(function=MLPNN_fitness_function, dimension=14, variable_type='real', \n",
    "           variable_type_mixed  = np.array(['int', 'real', 'int', 'int', 'int', 'int', 'int', 'int', 'int', 'int', 'int', 'int', 'int', 'real']) ,variable_boundaries=varbound, \n",
    "           algorithm_parameters=algorithm_param, function_timeout = 200)\n",
    "\n",
    "# 运行遗传算法进行优化\n",
    "model.run()\n",
    "\n",
    "# 获得优化的超参数组合\n",
    "best_params = model.output_dict['variable']\n",
    "\n",
    "\n",
    "'''\n",
    "# 使用优化的超参数训练最终的MLP回归模型\n",
    "learning_rate = best_params[0]\n",
    "hidden_layers = int(best_params[1])\n",
    "nodes_per_layer = int(best_params[2])\n",
    "batch_size = int(best_params[3])\n",
    "activation_function = ['logistic', 'relu', 'tanh'][int(best_params[4])]\n",
    "\n",
    "final_mlp = MLPRegressor(hidden_layer_sizes=(nodes_per_layer,) * hidden_layers,\n",
    "                         activation=activation_function,\n",
    "                         learning_rate_init=learning_rate,\n",
    "                         batch_size=batch_size,\n",
    "                         random_state=42)\n",
    "\n",
    "final_mlp.fit(X_train, y_train)\n",
    "\n",
    "# 使用测试数据评估最终的MLP回归模型性能（均方误差）\n",
    "y_pred_test = final_mlp.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"最优超参数组合：\", best_params)\n",
    "print(\"最终测试均方误差：\", test_mse)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_dict"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
